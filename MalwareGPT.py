import pandas as pd
from sklearn.metrics import accuracy_score
import torch
from transformers import GPT2Tokenizer, GPT2ForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tqdm.auto import tqdm

DATASET_PATH = 'MalwareMemoryDump.csv'
EPOCHS = 1

###### 1. Load the tokenizer and model ######
print('[+] Loading the tokenizer and model...')
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token 
model = GPT2ForSequenceClassification.from_pretrained('distilgpt2', num_labels=2)
model.config.pad_token_id = tokenizer.pad_token_id

####### 2. Load the dataset ######
print(f'[+] Loading the {DATASET_PATH} dataset...')
df = pd.read_csv(DATASET_PATH)

print('[+] Preprocessing dataset...')
print('> Encoding columns...')
# Convert categorical columns to numeric
label_encoder = LabelEncoder()
df['Label'] = label_encoder.fit_transform(df['Label'])  # Encode target column
df['Raw_Type'] = label_encoder.fit_transform(df['Raw_Type'])  # Encode categorical feature
df['SubType'] = label_encoder.fit_transform(df['SubType'])  # Encode categorical feature

# Convert each row of features into a single string
df['features_text'] = df.drop(columns=['Label']).apply(lambda row: ' '.join(row.astype(str)), axis=1)

print('> Splitting the dataset...')
# Split dataset
X_train, X_test, y_train, y_test = train_test_split(df['features_text'], df['Label'], test_size=0.1, random_state=42)


print('> Tokenizing the dataset...')
# Tokenize the dataset using GPT-2 tokenizer
def tokenize(batch):
    return tokenizer(batch['features_text'], padding=True, truncation=True, return_tensors="pt")

train_encodings = tokenizer(list(X_train), padding=True, truncation=True, return_tensors="pt")
test_encodings = tokenizer(list(X_test), padding=True, truncation=True, return_tensors="pt")

print('> Converting into tensors...')
# Convert labels to tensors
y_train_tensor = torch.tensor(y_train.values)
y_test_tensor = torch.tensor(y_test.values)

from torch.utils.data import Dataset, DataLoader

# Custom dataset class
class CustomTextDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item

print('> Creating DataLoader...')
# Create datasets and data loaders
train_dataset = CustomTextDataset(train_encodings, y_train_tensor)
test_dataset = CustomTextDataset(test_encodings, y_test_tensor)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

###### 5. Prepare optimizer and learning rate scheduler ######
print('[+] Preparing optimizer and learning rate scheduler...')
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_loader) * EPOCHS
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_training_steps // 2, gamma=0.1)

###### 6. Move the model to the GPU if available ######
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
print(f'> Model moved to: {device}')

###### 7. Training loop ######
print(f'[+] Training the model for {EPOCHS} epochs...')

progress_bar = tqdm(range(num_training_steps))
torch.cuda.empty_cache()

model.train()
for epoch in range(EPOCHS):
    print(f"> Training epoch {epoch + 1}/{EPOCHS}")
    for batch in train_loader:
        # Move input tensors to the device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        
        # Cast labels to LongTensor
        labels = batch['labels'].to(device).long()

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        
        # Backward pass
        loss.backward()
        
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

###### 8. Export the model ######
model_path = 'distilgpt2_trained_model.pth'
print(f"[+] Model exported to {model_path}")
torch.save(model.state_dict(), model_path)
