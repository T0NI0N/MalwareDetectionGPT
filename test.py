import torch
from transformers import GPT2Tokenizer, GPT2ForSequenceClassification
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import get_scheduler
from tqdm.auto import tqdm

# 1. Load the tokenizer and model
print('[+] Loading the tokenizer and model...')
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')


if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token 

model = GPT2ForSequenceClassification.from_pretrained('distilgpt2', num_labels=2)
model.config.pad_token_id = tokenizer.pad_token_id

# 2. Load the dataset
print('[+] Loading the dataset...')
dataset = load_dataset("imdb")

# 3. Tokenize the input data
print('[+] Tokenizing the input data...')
def tokenize_function(examples):
    return tokenizer(
        examples["text"], 
        padding="max_length", 
        truncation=True, 
        max_length=128, 
    )


tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 4. Create DataLoader for training and evaluation
print('[+] Creating DataLoader for training and evaluation...')
train_dataset = tokenized_datasets["train"]
test_dataset = tokenized_datasets["test"]

# Custom collate function to ensure tensors are returned from DataLoader
def collate_fn(batch):
    input_ids = torch.tensor([item['input_ids'] for item in batch], dtype=torch.long)
    attention_mask = torch.tensor([item['attention_mask'] for item in batch], dtype=torch.long)
    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long) 
    
    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}

# Apply collate_fn to the DataLoader
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=collate_fn)
test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)

# 5. Prepare optimizer and learning rate scheduler
print('[+] Preparing optimizer and learning rate scheduler...')
epochs = 1
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_dataloader) * epochs
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

# 6. Move the model to the GPU if available
print('[+] Moving the model to the GPU if available...')
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
print(f'> Model moved to: {device}')

# 7. Training loop
print(f'[+] Training the model for {epochs} epochs...')
progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(epochs): 
    for batch in train_dataloader:

        # Move the batch tensors to the device (GPU or CPU)
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        
        # Backward pass
        loss.backward()
        
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)